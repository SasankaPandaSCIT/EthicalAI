{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning with PySyft: A Practical Demonstration\n",
    "\n",
    "In this demonstration, we will explore the concept of Federated Learning using PySyft. We will simulate multiple clients that train a simple machine learning model on a dataset while keeping their data local. \n",
    "\n",
    "## Prerequisites\n",
    "Make sure you have the following libraries installed:\n",
    "- PySyft\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "\n",
    "You can install them using pip:\n",
    "```bash\n",
    "pip install syft torch numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries\n",
    "Let's start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import syft as sy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up the Federated Learning Environment\n",
    "\n",
    "We'll create a virtual environment with several clients. Each client will simulate a device that holds a subset of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hook to enable PySyft functionalities\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create two virtual workers (clients)\n",
    "client_1 = sy.VirtualWorker(hook, id=\"client_1\")\n",
    "client_2 = sy.VirtualWorker(hook, id=\"client_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating a Simple Dataset\n",
    "\n",
    "We'll create a synthetic dataset for a classification problem. In a real-world scenario, each client would have its own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "def create_data(num_samples):\n",
    "    # Features: Random data\n",
    "    x = np.random.rand(num_samples, 2).astype(np.float32)\n",
    "    # Labels: XOR pattern\n",
    "    y = (x[:, 0] > 0.5).astype(np.float32) ^ (x[:, 1] > 0.5).astype(np.float32)\n",
    "    return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# Generate data for each client\n",
    "x_client_1, y_client_1 = create_data(100)\n",
    "x_client_2, y_client_2 = create_data(100)\n",
    "\n",
    "# Send data to respective clients\n",
    "x_client_1 = x_client_1.send(client_1)\n",
    "y_client_1 = y_client_1.send(client_1)\n",
    "x_client_2 = x_client_2.send(client_2)\n",
    "y_client_2 = y_client_2.send(client_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Defining the Neural Network Model\n",
    "\n",
    "We will define a simple feedforward neural network for the classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the Model Locally on Each Client\n",
    "\n",
    "Each client will train the model locally on its own dataset for a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_client(model, x, y, client, epochs=10):\n",
    "    model.send(client)  # Send model to client\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(x).squeeze()\n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model.get()  # Get the model back to the server\n",
    "\n",
    "# Train the model on both clients\n",
    "model = train_model_on_client(model, x_client_1, y_client_1, client_1, epochs=10)\n",
    "model = train_model_on_client(model, x_client_2, y_client_2, client_2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Aggregating the Model Updates\n",
    "\n",
    "After training on both clients, we need to aggregate the model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating model weights from both clients\n",
    "def aggregate_models(models):\n",
    "    # Get the average of the weights from the client models\n",
    "    avg_model = models[0]  # Start with the first model\n",
    "    for model in models[1:]:\n",
    "        for param1, param2 in zip(avg_model.parameters(), model.parameters()):\n",
    "            param1.data += param2.data / len(models)\n",
    "    return avg_model\n",
    "\n",
    "# Simulate the aggregation\n",
    "model_aggregated = aggregate_models([model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluating the Aggregated Model\n",
    "\n",
    "Finally, let's evaluate the aggregated model on a synthetic test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset\n",
    "x_test, y_test = create_data(200)\n",
    "\n",
    "# Forward pass through the aggregated model\n",
    "with torch.no_grad():\n",
    "    predictions = model_aggregated(torch.tensor(x_test, dtype=torch.float32)).squeeze()\n",
    "    predictions = (predictions > 0.5).float()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == torch.tensor(y_test, dtype=torch.float32)).float().mean()\n",
    "print(f\"Accuracy of the aggregated model: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualization\n",
    "\n",
    "Let's visualize the data points and decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_decision_boundary(model, x, y):\n",
    "    x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "    y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    Z = model(torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32))\n",
    "    Z = (Z > 0.5).float().reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z.detach().numpy(), alpha=0.8)\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k')\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(model_aggregated, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demonstration, we successfully implemented a simple Federated Learning setup using PySyft. We trained a model on local datasets without sharing any sensitive data, illustrating the key benefits of Federated Learning for privacy.\n",
    "\n",
    "Feel free to modify the code, experiment with different datasets, and explore how Federated Learning can be applied to various problems!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
